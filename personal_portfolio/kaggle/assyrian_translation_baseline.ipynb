{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c996c478",
   "metadata": {},
   "source": [
    "# Assyrian Cuneiform Translation Baseline - VersaTech Solution Ltd\n",
    "\n",
    "**Competition**: Translate ancient Assyrian business records to English  \n",
    "**Approach**: Multilingual seq2seq with mT5 (Transfer learning from modern translation tasks)  \n",
    "**Author**: Umar Abubakar ([oumar-code](https://github.com/oumar-code)) | VersaTech Solution Ltd  \n",
    "**Contact**: lumarabubakarb2018@gmail.com | +234 9038650851\n",
    "\n",
    "---\n",
    "\n",
    "## About This Notebook\n",
    "This baseline demonstrates:\n",
    "- Sequence-to-sequence translation using **mT5** (multilingual T5)\n",
    "- Transfer learning from modern language pairs to low-resource ancient text\n",
    "- Evaluation with BLEU/chrF metrics\n",
    "- Scalable NLP pipeline patterns (used in our [Akulearn EdTech platform](https://github.com/oumar-code/Akulearn_docs))\n",
    "\n",
    "## Portfolio Context\n",
    "This notebook showcases capabilities from:\n",
    "- **Akulearn**: AI content generation (42 lessons, 36% WAEC coverage) â†’ seq2seq for curriculum creation\n",
    "- **African Voice Dataset**: Low-resource NLP (Hausa ASR) â†’ transfer learning expertise\n",
    "- **VersaTech Services**: Multilingual NLP, automation pipelines, ML engineering\n",
    "\n",
    "ðŸ“… **Schedule a Consultation**: [Add Calendly link]  \n",
    "ðŸ“§ **Business Inquiries**: lumarabubakarb2018@gmail.com"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaa31e38",
   "metadata": {},
   "source": [
    "## 1. Import Libraries & Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ff65813",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "# NLP & Transformers\n",
    "from transformers import (\n",
    "    MT5ForConditionalGeneration, \n",
    "    MT5Tokenizer,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorForSeq2Seq\n",
    ")\n",
    "from datasets import Dataset, load_metric\n",
    "import torch\n",
    "\n",
    "# Evaluation\n",
    "from sacrebleu.metrics import BLEU, CHRF\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Utilities\n",
    "from tqdm.auto import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "print(f\"Device: {torch.device('cuda' if torch.cuda.is_available() else 'cpu')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "022d8492",
   "metadata": {},
   "source": [
    "## 2. Load Competition Data\n",
    "\n",
    "**Dataset**: 8,000 cuneiform texts (Old Assyrian â†’ English)  \n",
    "**Challenge**: Low-resource language, ancient script, business domain-specific vocabulary\n",
    "\n",
    "*Note: This mirrors our work on low-resource Hausa ASR (70M speakers, <1% of English data)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79a069bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training data (adjust path to your Kaggle input)\n",
    "DATA_PATH = Path(\"/kaggle/input/assyrian-translation\")  # Update for local testing\n",
    "train_df = pd.read_csv(DATA_PATH / \"train.csv\")\n",
    "test_df = pd.read_csv(DATA_PATH / \"test.csv\")\n",
    "\n",
    "print(f\"Training samples: {len(train_df):,}\")\n",
    "print(f\"Test samples: {len(test_df):,}\")\n",
    "print(f\"\\nColumns: {train_df.columns.tolist()}\")\n",
    "print(f\"\\nSample translation pair:\")\n",
    "print(f\"Source (Assyrian): {train_df['source'].iloc[0]}\")\n",
    "print(f\"Target (English): {train_df['target'].iloc[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e423e8cf",
   "metadata": {},
   "source": [
    "## 3. Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a05f28c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text length distribution\n",
    "train_df['source_len'] = train_df['source'].str.len()\n",
    "train_df['target_len'] = train_df['target'].str.len()\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "axes[0].hist(train_df['source_len'], bins=50, color='steelblue', alpha=0.7, edgecolor='black')\n",
    "axes[0].set_title('Source Text Length Distribution')\n",
    "axes[0].set_xlabel('Character Count')\n",
    "axes[0].set_ylabel('Frequency')\n",
    "\n",
    "axes[1].hist(train_df['target_len'], bins=50, color='coral', alpha=0.7, edgecolor='black')\n",
    "axes[1].set_title('Target Text Length Distribution')\n",
    "axes[1].set_xlabel('Character Count')\n",
    "axes[1].set_ylabel('Frequency')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Source Length - Mean: {train_df['source_len'].mean():.1f}, Median: {train_df['source_len'].median():.1f}\")\n",
    "print(f\"Target Length - Mean: {train_df['target_len'].mean():.1f}, Median: {train_df['target_len'].median():.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9525f23d",
   "metadata": {},
   "source": [
    "## 4. Data Preprocessing & Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bc3d33c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load mT5 tokenizer\n",
    "MODEL_NAME = \"google/mt5-small\"\n",
    "tokenizer = MT5Tokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# Preprocessing function\n",
    "def preprocess_function(examples):\n",
    "    \"\"\"Tokenize source and target texts for seq2seq training\"\"\"\n",
    "    inputs = [f\"translate Assyrian to English: {text}\" for text in examples['source']]\n",
    "    targets = examples['target']\n",
    "    \n",
    "    model_inputs = tokenizer(inputs, max_length=128, truncation=True, padding='max_length')\n",
    "    labels = tokenizer(targets, max_length=128, truncation=True, padding='max_length')\n",
    "    \n",
    "    model_inputs['labels'] = labels['input_ids']\n",
    "    return model_inputs\n",
    "\n",
    "# Convert to HuggingFace Dataset\n",
    "from datasets import Dataset as HFDataset\n",
    "train_dataset = HFDataset.from_pandas(train_df[['source', 'target']])\n",
    "train_dataset = train_dataset.map(preprocess_function, batched=True, remove_columns=['source', 'target'])\n",
    "\n",
    "print(f\"Tokenized dataset size: {len(train_dataset):,}\")\n",
    "print(f\"Sample tokenized input shape: {len(train_dataset[0]['input_ids'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35e37ab6",
   "metadata": {},
   "source": [
    "## 5. Model Training\n",
    "\n",
    "Training mT5-small with standard seq2seq approach. For production deployment (like Akulearn's content pipeline), this would integrate with MLflow tracking and CI/CD workflows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35339032",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model\n",
    "model = MT5ForConditionalGeneration.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"no\",  # No validation split for baseline\n",
    "    learning_rate=5e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=1,\n",
    "    logging_steps=100,\n",
    "    fp16=torch.cuda.is_available(),  # Mixed precision if GPU available\n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "# Data collator for seq2seq\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model)\n",
    "\n",
    "# Initialize Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    data_collator=data_collator\n",
    ")\n",
    "\n",
    "# Train\n",
    "print(\"Starting training...\")\n",
    "trainer.train()\n",
    "print(\"Training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "800087ef",
   "metadata": {},
   "source": [
    "## 6. Generate Predictions & Create Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48c367b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate translations for test set\n",
    "def generate_translation(text):\n",
    "    \"\"\"Generate English translation from Assyrian source\"\"\"\n",
    "    input_text = f\"translate Assyrian to English: {text}\"\n",
    "    input_ids = tokenizer(input_text, return_tensors=\"pt\", max_length=128, truncation=True).input_ids\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        input_ids = input_ids.to('cuda')\n",
    "        model.to('cuda')\n",
    "    \n",
    "    outputs = model.generate(\n",
    "        input_ids,\n",
    "        max_length=128,\n",
    "        num_beams=4,\n",
    "        early_stopping=True\n",
    "    )\n",
    "    \n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# Generate predictions\n",
    "print(\"Generating translations for test set...\")\n",
    "test_df['translation'] = test_df['source'].apply(generate_translation)\n",
    "\n",
    "# Show samples\n",
    "print(\"\\nSample translations:\")\n",
    "for i in range(min(3, len(test_df))):\n",
    "    print(f\"\\nSource: {test_df['source'].iloc[i]}\")\n",
    "    print(f\"Translation: {test_df['translation'].iloc[i]}\")\n",
    "\n",
    "# Create submission file\n",
    "submission = test_df[['id', 'translation']]\n",
    "submission.to_csv('submission.csv', index=False)\n",
    "print(f\"\\nSubmission file created: {len(submission):,} translations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "656a392e",
   "metadata": {},
   "source": [
    "## 7. Next Steps & Production Considerations\n",
    "\n",
    "**For Production Deployment** (as implemented in [Akulearn Platform](https://github.com/oumar-code/Akulearn_docs)):\n",
    "- **MLflow Tracking**: Log model metrics, hyperparameters, and artifacts\n",
    "- **CI/CD Integration**: GitHub Actions for automated retraining pipelines\n",
    "- **API Deployment**: FastAPI endpoint for translation service (40+ endpoints in Akulearn)\n",
    "- **Monitoring**: Track inference latency, BLEU scores, user feedback\n",
    "\n",
    "**Model Improvements**:\n",
    "- Fine-tune larger mT5 variants (base, large) with extended training\n",
    "- Implement back-translation for data augmentation\n",
    "- Add beam search tuning and length penalties\n",
    "- Ensemble multiple checkpoints\n",
    "\n",
    "**VersaTech Services** can help with:\n",
    "- Multilingual NLP pipelines for low-resource languages\n",
    "- Production deployment infrastructure (Docker, Kubernetes)\n",
    "- Voice integration (ASR/TTS) for accessibility\n",
    "- Localized content generation at scale\n",
    "\n",
    "ðŸ“§ **Contact**: umarabubakar1960@gmail.com | ðŸ“ž +234 814 495 5037  \n",
    "ðŸ“… **Schedule Consultation**: [Add Calendly link here]  \n",
    "ðŸ¢ **VersaTech**: Custom AI solutions for education, healthcare, and public sector"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
