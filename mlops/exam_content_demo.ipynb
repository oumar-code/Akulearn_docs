{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5e17f856",
   "metadata": {},
   "source": [
    "## 1. Setup & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b39b42a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '/Users/hp/Documents/Akulearn_docs')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "from datetime import datetime, timedelta\n",
    "import json\n",
    "\n",
    "# Import our content generator\n",
    "from mlops.exam_content_generator import (\n",
    "    ExamContentOrchestrator,\n",
    "    GenerationRequest,\n",
    "    ExamBoard,\n",
    "    Difficulty,\n",
    ")\n",
    "\n",
    "print(\"‚úì All imports successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a7c3c90",
   "metadata": {},
   "source": [
    "## 2. Generate Content for Different Exam Boards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d6384cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize orchestrator\n",
    "orchestrator = ExamContentOrchestrator()\n",
    "\n",
    "# Generate questions for WAEC\n",
    "waec_request = GenerationRequest(\n",
    "    exam_board=ExamBoard.WAEC,\n",
    "    subject=\"mathematics\",\n",
    "    topic=\"algebra\",\n",
    "    difficulty=Difficulty.MEDIUM,\n",
    "    question_count=15\n",
    ")\n",
    "\n",
    "print(\"\\nüìù Generating WAEC Mathematics Content...\")\n",
    "waec_result = orchestrator.generate_content_batch(waec_request)\n",
    "print(f\"‚úì Generated {len(waec_result['generated'])} questions\")\n",
    "print(f\"‚úì {len(waec_result['validated'])} questions passed validation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8df56ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate questions for NECO\n",
    "neco_request = GenerationRequest(\n",
    "    exam_board=ExamBoard.NECO,\n",
    "    subject=\"biology\",\n",
    "    topic=\"photosynthesis\",\n",
    "    difficulty=Difficulty.EASY,\n",
    "    question_count=12\n",
    ")\n",
    "\n",
    "print(\"\\nüìù Generating NECO Biology Content...\")\n",
    "neco_result = orchestrator.generate_content_batch(neco_request)\n",
    "print(f\"‚úì Generated {len(neco_result['generated'])} questions\")\n",
    "print(f\"‚úì {len(neco_result['validated'])} questions passed validation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "101a4ec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate questions for JAMB\n",
    "jamb_request = GenerationRequest(\n",
    "    exam_board=ExamBoard.JAMB,\n",
    "    subject=\"chemistry\",\n",
    "    topic=\"periodic_table\",\n",
    "    difficulty=Difficulty.HARD,\n",
    "    question_count=18\n",
    ")\n",
    "\n",
    "print(\"\\nüìù Generating JAMB Chemistry Content...\")\n",
    "jamb_result = orchestrator.generate_content_batch(jamb_request)\n",
    "print(f\"‚úì Generated {len(jamb_result['generated'])} questions\")\n",
    "print(f\"‚úì {len(jamb_result['validated'])} questions passed validation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bde187e",
   "metadata": {},
   "source": [
    "## 3. Analyze Generation Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c05efcdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect all generated questions\n",
    "all_questions = waec_result['validated'] + neco_result['validated'] + jamb_result['validated']\n",
    "\n",
    "# Create DataFrame for analysis\n",
    "data = []\n",
    "for q in all_questions:\n",
    "    data.append({\n",
    "        'exam_board': q.exam_board.value.upper(),\n",
    "        'subject': q.subject,\n",
    "        'topic': q.topic,\n",
    "        'difficulty': q.difficulty.value,\n",
    "        'quality_score': q.quality_score,\n",
    "        'relevance_score': q.relevance_score,\n",
    "        'avg_score': (q.quality_score + q.relevance_score) / 2,\n",
    "    })\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "print(\"\\nüìä CONTENT GENERATION STATISTICS\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Total Questions Generated: {len(df)}\")\n",
    "print(f\"\\nBy Exam Board:\")\n",
    "print(df['exam_board'].value_counts())\n",
    "print(f\"\\nBy Subject:\")\n",
    "print(df['subject'].value_counts())\n",
    "print(f\"\\nBy Difficulty:\")\n",
    "print(df['difficulty'].value_counts())\n",
    "print(f\"\\nQuality Metrics:\")\n",
    "print(f\"  Avg Quality Score:   {df['quality_score'].mean():.3f}\")\n",
    "print(f\"  Avg Relevance Score: {df['relevance_score'].mean():.3f}\")\n",
    "print(f\"  Avg Overall Score:   {df['avg_score'].mean():.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d380823",
   "metadata": {},
   "source": [
    "## 4. Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "533766ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chart 1: Distribution by Exam Board\n",
    "fig1 = px.bar(\n",
    "    df['exam_board'].value_counts().reset_index().rename(columns={'count': 'Number of Questions', 'exam_board': 'Exam Board'}),\n",
    "    x='Exam Board',\n",
    "    y='Number of Questions',\n",
    "    color='Exam Board',\n",
    "    title='Questions Generated by Exam Board',\n",
    "    color_discrete_map={'WAEC': '#1f77b4', 'NECO': '#ff7f0e', 'JAMB': '#2ca02c'}\n",
    ")\n",
    "fig1.update_layout(showlegend=False, height=400)\n",
    "fig1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f08c081",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chart 2: Difficulty Distribution\n",
    "difficulty_order = ['easy', 'medium', 'hard']\n",
    "difficulty_data = df['difficulty'].value_counts().reindex(difficulty_order, fill_value=0)\n",
    "\n",
    "fig2 = px.pie(\n",
    "    values=difficulty_data.values,\n",
    "    names=difficulty_data.index,\n",
    "    title='Question Difficulty Distribution',\n",
    "    color_discrete_map={'easy': '#90EE90', 'medium': '#FFD700', 'hard': '#FF6B6B'}\n",
    ")\n",
    "fig2.update_layout(height=400)\n",
    "fig2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc8f9622",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chart 3: Quality Metrics by Exam Board\n",
    "quality_by_board = df.groupby('exam_board')[['quality_score', 'relevance_score']].mean().reset_index()\n",
    "\n",
    "fig3 = make_subplots(\n",
    "    rows=1, cols=2,\n",
    "    subplot_titles=(\"Average Quality Score\", \"Average Relevance Score\")\n",
    ")\n",
    "\n",
    "fig3.add_trace(\n",
    "    go.Bar(x=quality_by_board['exam_board'], y=quality_by_board['quality_score'], \n",
    "           name='Quality', marker_color='#1f77b4'),\n",
    "    row=1, col=1\n",
    ")\n",
    "\n",
    "fig3.add_trace(\n",
    "    go.Bar(x=quality_by_board['exam_board'], y=quality_by_board['relevance_score'], \n",
    "           name='Relevance', marker_color='#ff7f0e'),\n",
    "    row=1, col=2\n",
    ")\n",
    "\n",
    "fig3.update_layout(title_text=\"Quality Metrics by Exam Board\", height=400, showlegend=False)\n",
    "fig3.update_yaxes(range=[0, 1])\n",
    "fig3.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26b13d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chart 4: Subject Distribution\n",
    "fig4 = px.bar(\n",
    "    df['subject'].value_counts().reset_index().rename(columns={'count': 'Count', 'subject': 'Subject'}),\n",
    "    x='Subject',\n",
    "    y='Count',\n",
    "    title='Questions by Subject',\n",
    "    color='Subject'\n",
    ")\n",
    "fig4.update_layout(height=400, showlegend=False)\n",
    "fig4.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58aa16af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chart 5: Scatter plot - Quality vs Relevance\n",
    "fig5 = px.scatter(\n",
    "    df,\n",
    "    x='quality_score',\n",
    "    y='relevance_score',\n",
    "    color='exam_board',\n",
    "    size='avg_score',\n",
    "    hover_data=['difficulty'],\n",
    "    title='Quality vs Relevance Score Analysis',\n",
    "    labels={'quality_score': 'Quality Score', 'relevance_score': 'Relevance Score'},\n",
    "    color_discrete_map={'WAEC': '#1f77b4', 'NECO': '#ff7f0e', 'JAMB': '#2ca02c'}\n",
    ")\n",
    "fig5.update_layout(height=500)\n",
    "fig5.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b446e53e",
   "metadata": {},
   "source": [
    "## 5. Export Generated Content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dfb8229",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export all questions to JSON\n",
    "output_path = orchestrator.export_to_json(\n",
    "    all_questions,\n",
    "    'runs/exam_content_batch.json'\n",
    ")\n",
    "\n",
    "print(f\"\\n‚úì Exported {len(all_questions)} questions to: {output_path}\")\n",
    "\n",
    "# Show sample question\n",
    "sample_question = all_questions[0]\n",
    "print(f\"\\nüìå Sample Question:\")\n",
    "print(f\"  Exam Board: {sample_question.exam_board.value.upper()}\")\n",
    "print(f\"  Subject: {sample_question.subject}\")\n",
    "print(f\"  Topic: {sample_question.topic}\")\n",
    "print(f\"  Difficulty: {sample_question.difficulty.value}\")\n",
    "print(f\"\\n  Question: {sample_question.question_text}\")\n",
    "print(f\"\\n  Options:\")\n",
    "for i, opt in enumerate(sample_question.options, 1):\n",
    "    print(f\"    {i}. {opt}\")\n",
    "print(f\"\\n  Correct Answer: {sample_question.correct_answer}\")\n",
    "print(f\"  Quality Score: {sample_question.quality_score:.3f}\")\n",
    "print(f\"  Relevance Score: {sample_question.relevance_score:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0df1f41",
   "metadata": {},
   "source": [
    "## 6. Next Steps: Integration with Google Tools\n",
    "\n",
    "### Notebook LM Integration\n",
    "- Upload textbooks/study materials to Notebook LM\n",
    "- Generate audio study guides for selected topics\n",
    "- Embed audio URLs in quiz app\n",
    "\n",
    "### Google AI Studio Integration\n",
    "- Test improved prompts for question generation\n",
    "- Refine question templates\n",
    "- A/B test explanation styles\n",
    "\n",
    "### Hugging Face Hub\n",
    "- Fine-tune models on exam-specific data\n",
    "- Use better-performing models for generation\n",
    "- Deploy via Hugging Face Inference API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80a17c68",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nüéØ SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Total questions generated: {len(all_questions)}\")\n",
    "print(f\"Average quality score: {df['quality_score'].mean():.3f}/1.0\")\n",
    "print(f\"Average relevance score: {df['relevance_score'].mean():.3f}/1.0\")\n",
    "print(f\"\\nBy exam board:\")\n",
    "print(f\"  WAEC:  {len(waec_result['validated'])} questions\")\n",
    "print(f\"  NECO:  {len(neco_result['validated'])} questions\")\n",
    "print(f\"  JAMB:  {len(jamb_result['validated'])} questions\")\n",
    "print(f\"\\nNext: Train models, integrate with Google tools, scale to production\")\n",
    "print(\"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
