#!/usr/bin/env python3
"""
Standalone demo for exam content generation with visualizations
No external API calls - uses mock data for demonstration
"""

import json
import random
import pandas as pd
import plotly.express as px
import plotly.graph_objects as go
from plotly.subplots import make_subplots

print("="*70)
print("EXAM CONTENT GENERATION DEMO - VISUALIZATIONS")
print("="*70)

# ============================================================================
# 1. GENERATE MOCK DATA
# ============================================================================

def create_mock_questions(exam_board, subject, topic, difficulty, count):
    """Create mock question data for demo purposes"""
    questions = []
    
    for i in range(count):
        questions.append({
            'exam_board': exam_board,
            'subject': subject,
            'topic': topic,
            'difficulty': difficulty,
            'quality_score': random.uniform(0.75, 0.98),
            'relevance_score': random.uniform(0.80, 0.99),
        })
    
    return questions

print("\nüìù Generating Mock Content...")

# Generate questions for each exam board
waec_questions = create_mock_questions('WAEC', 'mathematics', 'algebra', 'medium', 15)
neco_questions = create_mock_questions('NECO', 'biology', 'photosynthesis', 'easy', 12)
jamb_questions = create_mock_questions('JAMB', 'chemistry', 'periodic_table', 'hard', 18)

all_questions = waec_questions + neco_questions + jamb_questions

# Create DataFrame
data = []
for q in all_questions:
    data.append({
        'exam_board': q['exam_board'],
        'subject': q['subject'],
        'topic': q['topic'],
        'difficulty': q['difficulty'],
        'quality_score': q['quality_score'],
        'relevance_score': q['relevance_score'],
        'avg_score': (q['quality_score'] + q['relevance_score']) / 2,
    })

df = pd.DataFrame(data)

# ============================================================================
# 2. STATISTICS
# ============================================================================

print("\nüìä CONTENT GENERATION STATISTICS")
print("="*70)
print(f"Total Questions Generated: {len(df)}")
print(f"\nBy Exam Board:")
print(df['exam_board'].value_counts())
print(f"\nBy Subject:")
print(df['subject'].value_counts())
print(f"\nBy Difficulty:")
print(df['difficulty'].value_counts())
print(f"\nQuality Metrics:")
print(f"  Avg Quality Score:   {df['quality_score'].mean():.3f}")
print(f"  Avg Relevance Score: {df['relevance_score'].mean():.3f}")
print(f"  Avg Overall Score:   {df['avg_score'].mean():.3f}")

# ============================================================================
# 3. VISUALIZATIONS
# ============================================================================

print("\nüìä Generating Visualizations...")

# Chart 1: Distribution by Exam Board
fig1 = px.bar(
    df['exam_board'].value_counts().reset_index().rename(
        columns={'count': 'Number of Questions', 'exam_board': 'Exam Board'}
    ),
    x='Exam Board',
    y='Number of Questions',
    color='Exam Board',
    title='Questions Generated by Exam Board',
    color_discrete_map={'WAEC': '#1f77b4', 'NECO': '#ff7f0e', 'JAMB': '#2ca02c'}
)
fig1.update_layout(showlegend=False, height=400)
fig1.write_html('runs/chart_1_exam_board_distribution.html')
print("  ‚úì Chart 1: Exam Board Distribution ‚Üí runs/chart_1_exam_board_distribution.html")

# Chart 2: Difficulty Distribution
difficulty_order = ['easy', 'medium', 'hard']
difficulty_data = df['difficulty'].value_counts().reindex(difficulty_order, fill_value=0)

fig2 = px.pie(
    values=difficulty_data.values,
    names=difficulty_data.index,
    title='Question Difficulty Distribution',
    color_discrete_map={'easy': '#90EE90', 'medium': '#FFD700', 'hard': '#FF6B6B'}
)
fig2.update_layout(height=400)
fig2.write_html('runs/chart_2_difficulty_distribution.html')
print("  ‚úì Chart 2: Difficulty Distribution ‚Üí runs/chart_2_difficulty_distribution.html")

# Chart 3: Quality Metrics by Exam Board
quality_by_board = df.groupby('exam_board')[['quality_score', 'relevance_score']].mean().reset_index()

fig3 = make_subplots(
    rows=1, cols=2,
    subplot_titles=("Average Quality Score", "Average Relevance Score")
)

fig3.add_trace(
    go.Bar(x=quality_by_board['exam_board'], y=quality_by_board['quality_score'], 
           name='Quality', marker_color='#1f77b4'),
    row=1, col=1
)

fig3.add_trace(
    go.Bar(x=quality_by_board['exam_board'], y=quality_by_board['relevance_score'], 
           name='Relevance', marker_color='#ff7f0e'),
    row=1, col=2
)

fig3.update_layout(title_text="Quality Metrics by Exam Board", height=400, showlegend=False)
fig3.update_yaxes(range=[0, 1])
fig3.write_html('runs/chart_3_quality_metrics.html')
print("  ‚úì Chart 3: Quality Metrics ‚Üí runs/chart_3_quality_metrics.html")

# Chart 4: Subject Distribution
fig4 = px.bar(
    df['subject'].value_counts().reset_index().rename(
        columns={'count': 'Count', 'subject': 'Subject'}
    ),
    x='Subject',
    y='Count',
    title='Questions by Subject',
    color='Subject'
)
fig4.update_layout(height=400, showlegend=False)
fig4.write_html('runs/chart_4_subject_distribution.html')
print("  ‚úì Chart 4: Subject Distribution ‚Üí runs/chart_4_subject_distribution.html")

# Chart 5: Scatter plot - Quality vs Relevance
fig5 = px.scatter(
    df,
    x='quality_score',
    y='relevance_score',
    color='exam_board',
    size='avg_score',
    hover_data=['difficulty'],
    title='Quality vs Relevance Score Analysis',
    labels={'quality_score': 'Quality Score', 'relevance_score': 'Relevance Score'},
    color_discrete_map={'WAEC': '#1f77b4', 'NECO': '#ff7f0e', 'JAMB': '#2ca02c'}
)
fig5.update_layout(height=500)
fig5.write_html('runs/chart_5_quality_vs_relevance.html')
print("  ‚úì Chart 5: Quality vs Relevance ‚Üí runs/chart_5_quality_vs_relevance.html")

# ============================================================================
# 4. EXPORT DATA
# ============================================================================

print("\nüíæ Exporting Data...")

output_path = 'runs/exam_content_batch.json'
with open(output_path, 'w') as f:
    json.dump([q for q in all_questions], f, indent=2)
print(f"  ‚úì Exported {len(all_questions)} questions to: {output_path}")

# Export statistics
stats = {
    'total_questions': len(all_questions),
    'by_exam_board': df['exam_board'].value_counts().to_dict(),
    'by_subject': df['subject'].value_counts().to_dict(),
    'by_difficulty': df['difficulty'].value_counts().to_dict(),
    'avg_quality_score': float(df['quality_score'].mean()),
    'avg_relevance_score': float(df['relevance_score'].mean()),
    'avg_overall_score': float(df['avg_score'].mean()),
}

with open('runs/content_generation_stats.json', 'w') as f:
    json.dump(stats, f, indent=2)
print(f"  ‚úì Exported statistics to: runs/content_generation_stats.json")

# ============================================================================
# 5. SUMMARY
# ============================================================================

print("\n" + "="*70)
print("üéØ SUMMARY")
print("="*70)
print(f"Total questions generated: {len(all_questions)}")
print(f"Average quality score: {df['quality_score'].mean():.3f}/1.0")
print(f"Average relevance score: {df['relevance_score'].mean():.3f}/1.0")
print(f"\nBy exam board:")
print(f"  WAEC:  {len(waec_questions)} questions")
print(f"  NECO:  {len(neco_questions)} questions")
print(f"  JAMB:  {len(jamb_questions)} questions")
print(f"\nüìÅ Outputs:")
print(f"  ‚úì 5 Interactive HTML visualizations in runs/")
print(f"  ‚úì Question data: runs/exam_content_batch.json")
print(f"  ‚úì Statistics: runs/content_generation_stats.json")
print("\nüí° Next Steps:")
print("  1. Open visualization files in browser to explore interactive charts")
print("  2. Integrate with Google Notebook LM for audio study guides")
print("  3. Deploy API with FastAPI to serve generated content")
print("  4. Scale to production with MLflow tracking")
print("="*70 + "\n")
